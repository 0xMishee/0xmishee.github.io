---
layout: post
title: "When Support Replaces Blame, Safety Thrives"
date: 25-11-13 23:00:00 +0200  
categories: [OT]
tags: [safety]
description: "Building safe system through the use of safe environments"
comments: false
---

This is an odd one for me, normally, you’ll find me talking about technology down to the bits. But perhaps it’s the holiday spirit; I thought I’d share a good book and some reflections on the space many of us work in.

This year, I’ve been focusing on OT/ICS and improving my ability to navigate that world, from culture and architecture to domain-specific challenges and technologies. Which has included working toward my master’s degree, building my own ICS network lab, writing a few experimental projects (some of which I’ve shared publicly), and reading my fair share of books and academic papers. 

Most of my growth came during a month long vacation when I finally got the time to read _Engineering a Safer World: Systems Thinking Applied to Safety_ by Nancy G. Leveson. Yes, the book just oozes raw sex appeal, I know.

It’s filled with all the thrilling topics you’d expect from your ideal vacation read: _A Systems-Theoretic View of Causality_, _Integrating Safety into System Engineering_, and everyone’s favourite, _Physical Process Failures and Dysfunctional Integrations._ Beneath that dense layer of mental tar that can clog your brain if you read too much in one sitting, there’s a small but striking chapter on Safety Culture.

The chapter struck me more than what would typically tickle my fancy, probably because of the conversations I’ve had with people over the years, at conferences, talks, and over one too many beers. It reminded me of those all-too-familiar discussions where the same frustrations keep surfacing, no matter who you talk to or what organisation they’re from.

When someone first starts ranting about their management being uninterested, uninvolved, or uninformed about safety and security, and sometimes even seemingly making things worse, you tend to joke about it. You tell them to hang in there, maybe suggest looking for another position elsewhere. We’re all used to being understaffed and overworked; it’s part of the game. None of this is exactly new. 

But when that one-off interaction turns into a common topic of conversation with just about everyone in the field, you start to realize there’s a systemic issue within our sector.

Nancy makes it clear right from the start: effectively achieving any of the safety goals discussed earlier in the book ultimately depends on management. She references a paper by Urban Kjellen, _“Deviations and the Feedback Control of Accidents,”_ noting that _“management commitment to the safety goals is the most important factor distinguishing safe from unsafe systems and companies. Poor management decision-making can undermine any attempts to improve safety and ensure that accidents continue to occur.”_

In other words, no matter how passionate or hardworking you are, no matter how much you push for improvements and better safety, if management isn’t competent or committed, you’re going to have a rough time and from earlier conversations might even harm your employment.

The information on how to build a strong safety culture, ensure robust security, and develop reliable systems is widely available and not limited to those who grew up with the demo scene or stumbled upon some obscure archive. Yet we continue to see this kind of behaviour from people in senior positions who appear passionate about the field and have decades of experience. Their enthusiasm, however, often seems less about genuine engagement and more about projecting a particular image. Phrases like “don’t worry, they like or know technology” are commonly used to reassure others about management’s competence. The concern is not with actual competence but with the perception of it, the carefully curated image that is broadcasted.

Nancy argues that you cannot sloganeer your way to an effective safety culture. Stories without substance remain just that, and ad-hoc creation of policies, goals, missions, standard operating procedures, or other fancy terms does not inherently reduce risk. While such measures may have short-term effects, superficial fixes that do not address the underlying shared values and social norms are very likely to be undone over time.

She brings up some stories of incidents I believe most people have heard, but perhaps didn't look into particular well, me included. One being the Columbia accident that involved the NASA space shuttle to disintegrate as it re-entered the atmosphere and caused seven people to loose their lives was due insulating foam had torn from the tank and struct one of its wings. The agency released CAIB ( Columbia Accident Investigation Board Reporting ) specifying amongst other things that managers didn't listen to the engineers concerns, they've effectively had made barriers against dissenting opinions; listening only to what they wanted to hear. 

Another example is the train derailment accident in Fukuchiyama, Japan, in 2005. The derailment occurred because the train driver was on the phone to make sure he wouldn't be reported for a minor infraction, leaving him unaware of an approaching curve. As a result, the train did not slow down, causing the deaths of 106 people and injuries to another 562.

A meaningless and harmful culture set by those in charge can create a hostile work environment, putting even the unaware and uninvolved at risk, and worst case scenarios, losing their lives. Most of us are fortunate not to be part of such extreme scenarios, yet we are sometimes part of the environments that foster them. In the Fukuchiyama case, the train driver feared being reported for minor mistakes, while the engineers did not feel safe expressing their concerns or opinions.

When culture turns its focus away from people, it starts to lose its purpose. What was meant to support and protect ends up serving the broadcasted image. The stories, the reports, and the numbers take centre stage, while the real work and the real people fade into the background. That is when culture becomes hollow, when it exists more on paper than in practice.

Companies have, to their credit, created tools and systems that allow employees to raise concerns, whether through whistle-blower functions, internal surveys, reporting systems, or open dialogue meetings. However, these are only tools, and they depend on a healthy and functional culture to work as intended. Without that foundation, false or misleading information can find its way into these systems. This creates a dilemma of moderation, where one assumes that the collective reasoning of many will balance out and provide an accurate picture of the organization’s situation. In reality, the data becomes distorted by the very cultural weaknesses that the systems were designed to improve.

Nancy believes that "a culture of blame creates a climate of fear that makes people reluctant to share information." and that simply changing people wouldn't improve the system. Culture operates on a system level while tools that we implement operates on a component level. 

True improvement comes not from pointing fingers, but from designing systems that encourage openness, learning, and trust. When people feel safe to speak up about mistakes and near misses, organizations can uncover the real causes of failure and prevent them from happening again. In this sense, accountability should empower, not punish, the people within the system.

In the end, blame is the enemy of safety. 